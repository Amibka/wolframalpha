"""
Inference: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ SymPy –∫–æ–¥
"""
import torch
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent))

from config import *
from model.seq2seq import create_model
from tokenizer.math_tokenizer import MathTokenizer
from preprocessing.text_normalizer import TextNormalizer


class MathTranslator:
    def __init__(self, model_path, vocab_path):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        self.tokenizer = MathTokenizer(vocab_path)
        self.normalizer = TextNormalizer()

        # –°–æ–∑–¥–∞—ë–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = create_model(MODEL_CONFIG, self.tokenizer.vocab_size, self.tokenizer.vocab_size)
        checkpoint = torch.load(model_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model = self.model.to(self.device)
        self.model.eval()

        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
        print(f"üìè –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {self.tokenizer.vocab_size}")

    def translate(self, text, max_length=128, beam_size=5):
        """
        –ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –≤ SymPy –∫–æ–¥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º beam search

        Args:
            text: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç (RU/EN)
            max_length: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≤—ã—Ö–æ–¥–∞
            beam_size: —Ä–∞–∑–º–µ—Ä beam –¥–ª—è beam search

        Returns:
            sympy_code: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SymPy –∫–æ–¥
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        text = self.normalizer.normalize(text)

        # –ö–æ–¥–∏—Ä—É–µ–º –≤—Ö–æ–¥
        src = torch.LongTensor(self.tokenizer.encode(text, max_length=max_length)).unsqueeze(0)
        src = src.to(self.device)

        with torch.no_grad():
            # –ö–æ–¥–∏—Ä—É–µ–º source
            encoder_output = self.model.encode(src)
            src_mask = self.model.make_src_mask(src)

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è beam search
            beams = [(torch.LongTensor([[SPECIAL_TOKENS["SOS"]]]).to(self.device), 0.0)]

            for _ in range(max_length):
                new_beams = []

                for beam, score in beams:
                    if beam[0, -1].item() == SPECIAL_TOKENS["EOS"]:
                        new_beams.append((beam, score))
                        continue

                    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ–¥–∏–Ω —à–∞–≥
                    output = self.model.decode(beam, encoder_output, src_mask)
                    logits = output[:, -1, :]
                    log_probs = torch.log_softmax(logits, dim=-1)

                    # –ë–µ—Ä—ë–º top-k
                    top_log_probs, top_indices = log_probs.topk(beam_size)

                    for i in range(beam_size):
                        token = top_indices[0, i].unsqueeze(0).unsqueeze(0)
                        new_beam = torch.cat([beam, token], dim=1)
                        new_score = score + top_log_probs[0, i].item()
                        new_beams.append((new_beam, new_score))

                # –û—Å—Ç–∞–≤–ª—è–µ–º top beam_size
                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]

                # –ï—Å–ª–∏ –≤—Å–µ beams –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å, –≤—ã—Ö–æ–¥–∏–º
                if all(beam[0, -1].item() == SPECIAL_TOKENS["EOS"] for beam, _ in beams):
                    break

            # –ë–µ—Ä—ë–º –ª—É—á—à–∏–π beam
            best_beam, _ = beams[0]
            output_ids = best_beam[0].tolist()

            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º
            sympy_code = self.tokenizer.decode(output_ids)

        return sympy_code

    def batch_translate(self, texts, max_length=128):
        """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç batch —Ç–µ–∫—Å—Ç–æ–≤"""
        return [self.translate(text, max_length) for text in texts]


def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã"""
    model_path = CHECKPOINTS_DIR / "best_model.pt"

    if not model_path.exists():
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
        print("–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å: python train.py")
        return

    translator = MathTranslator(model_path, VOCAB_PATH)

    # –ü—Ä–∏–º–µ—Ä—ã
    test_cases = [
        "—Ä–µ—à–∏ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ 4x + 1 = 10",
        "–ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è sin(x^2)",
        "–∏–Ω—Ç–µ–≥—Ä–∞–ª –æ—Ç 0 –¥–æ 5 x + 1",
        "—É–ø—Ä–æ—Å—Ç–∏ (x^2 + 2x + 1)/(x + 1)",
        "—Ä–∞–∑–ª–æ–∂–∏ –Ω–∞ –º–Ω–æ–∂–∏—Ç–µ–ª–∏ x^2 - 4",
        "–ø—Ä–µ–¥–µ–ª sin(x)/x –ø—Ä–∏ x -> 0",
        "solve equation 2x - 5 = 0",
        "derivative of cos(x^2)",
        "simplify (x + 1)^2",
    ]

    print("\n" + "=" * 60)
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò")
    print("=" * 60 + "\n")

    for i, text in enumerate(test_cases, 1):
        sympy_code = translator.translate(text)
        print(f"{i}. –í—Ö–æ–¥: {text}")
        print(f"   –í—ã—Ö–æ–¥: {sympy_code}\n")


if __name__ == "__main__":
    main()