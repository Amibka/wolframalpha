"""
–°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è Seq2Seq –º–æ–¥–µ–ª–∏
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
from pathlib import Path
from tqdm import tqdm
import sys

sys.path.append(str(Path(__file__).parent))

from config import *
from model.seq2seq import create_model
from tokenizer.math_tokenizer import MathTokenizer


class MathDataset(Dataset):
    def __init__(self, data, tokenizer, max_len=128):
        self.data = data
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data[idx]

        # –ö–æ–¥–∏—Ä—É–µ–º –≤—Ö–æ–¥ –∏ –≤—ã—Ö–æ–¥
        src = self.tokenizer.encode(sample["input"], max_length=self.max_len)
        tgt = self.tokenizer.encode(sample["output"], max_length=self.max_len)

        # –î–æ–±–∞–≤–ª—è–µ–º [SOS] –∏ [EOS] –∫ target
        tgt_input = [SPECIAL_TOKENS["SOS"]] + tgt[:-1]
        tgt_output = tgt + [SPECIAL_TOKENS["EOS"]]

        return {
            "src": torch.LongTensor(src),
            "tgt_input": torch.LongTensor(tgt_input),
            "tgt_output": torch.LongTensor(tgt_output)
        }


def train_epoch(model, dataloader, optimizer, criterion, device):
    model.train()
    total_loss = 0

    progress_bar = tqdm(dataloader, desc="Training")

    for batch in progress_bar:
        src = batch["src"].to(device)
        tgt_input = batch["tgt_input"].to(device)
        tgt_output = batch["tgt_output"].to(device)

        optimizer.zero_grad()

        # Forward pass
        output = model(src, tgt_input)

        # Reshape –¥–ª—è loss
        output = output.reshape(-1, output.shape[-1])
        tgt_output = tgt_output.reshape(-1)

        # Loss
        loss = criterion(output, tgt_output)

        # Backward
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        total_loss += loss.item()
        progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})

    return total_loss / len(dataloader)


def evaluate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            src = batch["src"].to(device)
            tgt_input = batch["tgt_input"].to(device)
            tgt_output = batch["tgt_output"].to(device)

            output = model(src, tgt_input)
            output = output.reshape(-1, output.shape[-1])
            tgt_output = tgt_output.reshape(-1)

            loss = criterion(output, tgt_output)
            total_loss += loss.item()

    return total_loss / len(dataloader)


def main():
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Math NLP ‚Üí SymPy\n")

    # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üì± –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\n")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {TRAINING_DATA_PATH}")
    with open(TRAINING_DATA_PATH, 'r', encoding='utf-8') as f:
        dataset = json.load(f)

    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val
    split_idx = int(len(dataset) * TRAINING_CONFIG["train_split"])
    train_data = dataset[:split_idx]
    val_data = dataset[split_idx:]

    print(f"üìä Train: {len(train_data)}, Val: {len(val_data)}\n")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    print(f"üî§ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {VOCAB_PATH}")
    tokenizer = MathTokenizer(VOCAB_PATH)
    print(f"üìè –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {tokenizer.vocab_size}\n")

    # –î–∞—Ç–∞—Å–µ—Ç—ã –∏ dataloaders
    train_dataset = MathDataset(train_data, tokenizer, MODEL_CONFIG["max_seq_length"])
    val_dataset = MathDataset(val_data, tokenizer, MODEL_CONFIG["max_seq_length"])

    train_loader = DataLoader(
        train_dataset,
        batch_size=TRAINING_CONFIG["batch_size"],
        shuffle=True,
        num_workers=4
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=TRAINING_CONFIG["batch_size"],
        shuffle=False,
        num_workers=4
    )

    # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å
    print("üß† –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = create_model(MODEL_CONFIG, tokenizer.vocab_size, tokenizer.vocab_size)
    model = model.to(device)

    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ loss
    optimizer = optim.Adam(model.parameters(), lr=TRAINING_CONFIG["learning_rate"])
    criterion = nn.CrossEntropyLoss(ignore_index=SPECIAL_TOKENS["PAD"])

    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞: {sum(p.numel() for p in model.parameters())} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n")

    # –û–±—É—á–µ–Ω–∏–µ
    best_val_loss = float('inf')

    for epoch in range(TRAINING_CONFIG["num_epochs"]):
        print(f"\n{'=' * 50}")
        print(f"Epoch {epoch + 1}/{TRAINING_CONFIG['num_epochs']}")
        print(f"{'=' * 50}")

        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
        val_loss = evaluate(model, val_loader, criterion, device)

        print(f"\nüìä Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ checkpoint
        if (epoch + 1) % TRAINING_CONFIG["save_every"] == 0 or val_loss < best_val_loss:
            checkpoint_path = CHECKPOINTS_DIR / f"model_epoch_{epoch + 1}.pt"
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
            }, checkpoint_path)
            print(f"üíæ Checkpoint —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {checkpoint_path}")

            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_path = CHECKPOINTS_DIR / "best_model.pt"
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'val_loss': val_loss,
                }, best_path)
                print(f"‚≠ê –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {best_path}")

    print("\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")


if __name__ == "__main__":
    main()