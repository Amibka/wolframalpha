"""
–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
"""
import json
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
from tokenizer.math_tokenizer import MathTokenizer
from config import TRAINING_DATA_PATH, VOCAB_PATH

def build_vocabulary():
    """–°—Ç—Ä–æ–∏—Ç —Å–ª–æ–≤–∞—Ä—å –∏–∑ training_data.json"""
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
    if not TRAINING_DATA_PATH.exists():
        print(f"‚ùå –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {TRAINING_DATA_PATH}")
        print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python utils/dataset_generator.py")
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {TRAINING_DATA_PATH}")
    with open(TRAINING_DATA_PATH, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã (–≤—Ö–æ–¥—ã –∏ –≤—ã—Ö–æ–¥—ã)
    all_texts = []
    for sample in dataset:
        all_texts.append(sample["input"])
        all_texts.append(sample["output"])
    
    print(f"üìä –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(all_texts)}")
    
    # –°—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å
    tokenizer = MathTokenizer()
    tokenizer.build_vocab(all_texts, min_freq=2)
    
    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    VOCAB_PATH.parent.mkdir(parents=True, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    tokenizer.save_vocab(VOCAB_PATH)
    print(f"üíæ –°–ª–æ–≤–∞—Ä—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {VOCAB_PATH}")
    print(f"üìè –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {tokenizer.vocab_size} —Ç–æ–∫–µ–Ω–æ–≤")

if __name__ == "__main__":
    build_vocabulary()